<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">
  <link rel="alternate" href="/atom.xml" title="Snow" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","width":300,"display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>
  <meta name="description" content="1、设置MNIST数据 2、从头构建神经网络（不使用 torch.nn ） 3、使用 torch.nn.functional 4、使用 nn.Module 重构 5、使用 nn.Linear 重构 6、使用 optim 重构 7、使用 Dataset 重构 8、使用 DataLoader 重构 9、增加验证 10、创建 fit() 和 get_data() 11、总结">
<meta name="keywords" content="转载,python,pytorch">
<meta property="og:type" content="article">
<meta property="og:title" content="torch.nn学习(1)">
<meta property="og:url" content="https://behappy00.github.io/2021/04/16/torch-nn学习-1/index.html">
<meta property="og:site_name" content="Snow">
<meta property="og:description" content="1、设置MNIST数据 2、从头构建神经网络（不使用 torch.nn ） 3、使用 torch.nn.functional 4、使用 nn.Module 重构 5、使用 nn.Linear 重构 6、使用 optim 重构 7、使用 Dataset 重构 8、使用 DataLoader 重构 9、增加验证 10、创建 fit() 和 get_data() 11、总结">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190828233837271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NwcmluZ18yNA==,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2023-04-03T13:21:07.346Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="torch.nn学习(1)">
<meta name="twitter:description" content="1、设置MNIST数据 2、从头构建神经网络（不使用 torch.nn ） 3、使用 torch.nn.functional 4、使用 nn.Module 重构 5、使用 nn.Linear 重构 6、使用 optim 重构 7、使用 Dataset 重构 8、使用 DataLoader 重构 9、增加验证 10、创建 fit() 和 get_data() 11、总结">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20190828233837271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NwcmluZ18yNA==,size_16,color_FFFFFF,t_70">
  <link rel="canonical" href="https://behappy00.github.io/2021/04/16/torch-nn学习-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>torch.nn学习(1) | Snow</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Snow</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">光而不耀</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/behappy00" class="github-corner" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://behappy00.github.io/2021/04/16/torch-nn学习-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Snow">
      <meta itemprop="description" content="计算机 Hiter">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Snow">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">torch.nn学习(1)

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2021-04-16 10:56:20" itemprop="dateCreated datePublished" datetime="2021-04-16T10:56:20+08:00">2021-04-16</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-03 21:21:07" itemprop="dateModified" datetime="2023-04-03T21:21:07+08:00">2023-04-03</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/学习笔记/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2021/04/16/torch-nn学习-1/" class="post-meta-item leancloud_visitors" data-flag-title="torch.nn学习(1)" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/04/16/torch-nn学习-1/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2021/04/16/torch-nn学习-1/" itemprop="commentCount"></span></a>
  </span>
  
  
          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              
                <span class="post-meta-item-text">本文字数：</span>
              
              <span>14k</span>
            </span>
          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              
              <span>13 分钟</span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <hr><ul>
<li><a href="#1设置mnist数据">1、设置MNIST数据</a></li>
<li><a href="#2从头构建神经网络不使用-torchnn-">2、从头构建神经网络（不使用 <code>torch.nn</code> ）</a></li>
<li><a href="#3使用-torchnnfunctional">3、使用 <code>torch.nn.functional</code></a></li>
<li><a href="#4使用-nnmodule-重构">4、使用 <code>nn.Module</code> 重构</a></li>
<li><a href="#5使用-nnlinear-重构">5、使用 <code>nn.Linear</code> 重构</a></li>
<li><a href="#6使用-optim-重构">6、使用 <code>optim</code> 重构</a></li>
<li><a href="#7使用-dataset-重构">7、使用 <code>Dataset</code> 重构</a></li>
<li><a href="#8使用-dataloader-重构">8、使用 <code>DataLoader</code> 重构</a></li>
<li><a href="#9增加验证">9、增加验证</a></li>
<li><a href="#10创建-fit-和-get_data">10、创建 <code>fit()</code> 和 <code>get_data()</code></a></li>
<li><a href="#11总结">11、总结</a></li>
</ul><a id="more"></a>
<!-- TOC -->

<!-- /TOC -->
<hr>
<p><code>PyTorch</code> 提供了设计优雅的模块和类： <code>torch.nn</code>， <code>torch.optim</code>， <code>Dateset</code> 和 <code>DataLoader</code>，以帮助你创建和训练神经网络。为了逐渐理解，我们首先在 <code>MNIST</code> 数据集上训练基本的神经网络，而不使用这些模块的任何特征。最初只会使用最基本的 PyTorch tensor 功能。然后，我们逐步添加来自 <code>torch.nn</code>， <code>torch.optim</code>， <code>Dataset</code> 和 <code>DataLoader</code> 的一个特征，以显示每一部分的功能，以及它如何使得代码更简洁或灵活。</p>
<h3 id="1、设置MNIST数据"><a href="#1、设置MNIST数据" class="headerlink" title="1、设置MNIST数据"></a>1、设置MNIST数据</h3><p>我们将使用经典的 <code>MNIST</code> 数据集，该数据集由手写数字（0-9）的黑白图像组成。</p>
<pre><code>MNIST 数据集它包含了四个部分:
Training set images: train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本)
Training set labels: train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签)
Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本)
Test set labels: t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签)
</code></pre><p>我们将使用 <code>pathlib</code> 来处理路径（Python3标准库的一部分），用 <code>requests</code> 下载数据。只有当我们需要模块的时候才会导入它们，因此你可以清楚地看到正在使用的模块。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from pathlib import Path</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">DATA_PATH = Path(&quot;data&quot;)</span><br><span class="line">PATH = DATA_PATH / &quot;mnist&quot;</span><br><span class="line"></span><br><span class="line">PATH.mkdir(parents=True, exist_ok=True)</span><br><span class="line"></span><br><span class="line">URL = &quot;http://deeplearning.net/data/mnist/&quot;</span><br><span class="line">FILENAME = &quot;mnist.pkl.gz&quot;</span><br><span class="line"></span><br><span class="line">if not (PATH / FILENAME).exists():</span><br><span class="line">        content = requests.get(URL + FILENAME).content</span><br><span class="line">        (PATH / FILENAME).open(&quot;wb&quot;).write(content)</span><br></pre></td></tr></table></figure>
<p>该数据集的格式为 <code>NumPy array</code>，使用 <code>pickle</code> 存储。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pickle</span><br><span class="line">import gzip</span><br><span class="line"></span><br><span class="line">with gzip.open((PATH / FILENAME).as_posix(), &quot;rb&quot;) as f:</span><br><span class="line">        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=&quot;latin-1&quot;)</span><br></pre></td></tr></table></figure>
<p>每个图片大小为28x28，并存储为长度为784（=28x28）的扁平行。</p>
<p>查看其中的一个图片：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib import pyplot</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">pyplot.imshow(x_train[0].reshape((28, 28)), cmap=&quot;gray&quot;)</span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure>
<p>输出为：<br><img src="https://img-blog.csdnimg.cn/20190828233837271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NwcmluZ18yNA==,size_16,color_FFFFFF,t_70" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(50000, 784)</span><br></pre></td></tr></table></figure>
<p>PyTorch使用 <code>tensor</code> 而不是 NumPy <code>array</code>，所以我们需要将其转换。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x_train, y_train, x_valid, y_valid = map(</span><br><span class="line">    torch.tensor, (x_train, y_train, x_valid, y_valid)</span><br><span class="line">)</span><br><span class="line">n, c = x_train.shape</span><br><span class="line">x_train, x_train.shape, y_train.min(), y_train.max()</span><br><span class="line">print(x_train, y_train)</span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(y_train.min(), y_train.max())</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0., 0., 0.,  ..., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0.,  ..., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0.,  ..., 0., 0., 0.],</span><br><span class="line">        ...,</span><br><span class="line">        [0., 0., 0.,  ..., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0.,  ..., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])</span><br><span class="line">torch.Size([50000, 784])</span><br><span class="line">tensor(0) tensor(9)</span><br></pre></td></tr></table></figure>
<h3 id="2、从头构建神经网络（不使用-torch-nn-）"><a href="#2、从头构建神经网络（不使用-torch-nn-）" class="headerlink" title="2、从头构建神经网络（不使用 torch.nn ）"></a>2、从头构建神经网络（不使用 <code>torch.nn</code> ）</h3><p>我们首先只使用PyTorch <code>tensor</code> 操作创建一个模型。PyTorch提供了创建随机tensor或零填充tensor的方法，我们将使用这些方法为简单线性模型创建权重（weight）和偏置值（bias）。它们都是普通的tensor，除此之外，我们增加了一点：我们告诉PyTorch它们需要梯度。这将使得PyTorch记录tensor上的所有操作，因此PyTorch可以在反向传播中自动计算梯度。</p>
<p>对于权重，我们在初始化之后设置 <code>requires_grad</code>，因为我们不想在梯度中包含这一步。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">###initializing the weights with Xavier initialisation (by multiplying with 1/sqrt(n)).</span><br><span class="line"></span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line">weights = torch.randn(784, 10) / math.sqrt(784)</span><br><span class="line">weights.requires_grad_()</span><br><span class="line">bias = torch.zeros(10, requires_grad=True)</span><br></pre></td></tr></table></figure>
<p>由于PyTorch可以自动计算梯度，我们可以使用任何标准Python函数（或可调用对象）作为模型。因此我们只编写一个矩阵乘法和广播加法来创建一个简单的线性模型。我们还需要一个激活函数，因此我们将编写 <code>log_softmax</code> 并使用它。记住：虽然PyTorch提供了许多的预先编写好的损失函数、激活函数等，但是你可以使用普通的Python编写自己的函数。PyTorch甚至可以自动为你的函数创建快速GPU或矢量化CPU代码。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def log_softmax(x):</span><br><span class="line">    return x - x.exp().sum(-1).log().unsqueeze(-1)</span><br><span class="line"></span><br><span class="line">def model(xb):</span><br><span class="line">    return log_softmax(xb @ weights + bias)</span><br></pre></td></tr></table></figure>
<p>上面的代码中，”@”代表点积操作。我们将会在一批数据（64个图片）上调用我们编写的函数。这是一个前向传播。注意，现阶段我们的预测情况比随机猜测好不到哪里，因为我们是从随机权重开始的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bs = 64                  ### batch size</span><br><span class="line"></span><br><span class="line">xb = x_train[0:bs]       ### a mini-batch from x</span><br><span class="line">preds = model(xb)        ### predictions</span><br><span class="line">preds[0], preds.shape</span><br><span class="line">print(preds[0], preds.shape)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([-1.7022, -3.0342, -2.4138, -2.6452, -2.7764, -2.0892, -2.2945, -2.5480,</span><br><span class="line">        -2.3732, -1.8915], grad_fn=&lt;selectbackward&gt;) torch.Size([64, 10])</span><br><span class="line">&lt;/selectbackward&gt;</span><br></pre></td></tr></table></figure>
<p>如你所见，pred tensor不只包含tensor值，还包含一个梯度函数，我们随后将会使用它来来进行反向传播。</p>
<p>我们编写负对数似然函数并将其用作损失函数（我们可以只使用标准的Python实现）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def nll(input, target):</span><br><span class="line">    return -input[range(target.shape[0]), target].mean()</span><br><span class="line"></span><br><span class="line">loss_func = nll</span><br></pre></td></tr></table></figure>
<p>查看随机初始化模型的损失，这样我们随后就可以看到在使用了反向传播后，是否改进了模型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yb = y_train[0:bs]</span><br><span class="line">print(loss_func(preds, yb))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(2.3783, grad_fn=&lt;negbackward&gt;)</span><br><span class="line">&lt;/negbackward&gt;</span><br></pre></td></tr></table></figure>
<p>编写函数计算模型的精确度。对于每一次预测，如果最大值的索引和目标值匹配，则表示预测正确。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def accuracy(out, yb):</span><br><span class="line">    preds = torch.argmax(out, dim=1)</span><br><span class="line">    return (preds == yb).float().mean()</span><br></pre></td></tr></table></figure>
<p>查看随机初始化模型的精确度，因此我们可以观察随着损失的改善，精确度是否提升。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(accuracy(preds, yb))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.0938)</span><br></pre></td></tr></table></figure>
<p>现在我们可以进行训练。对于每次迭代，将会做以下几件事：</p>
<ul>
<li>选择一批数据（mini-batch）</li>
<li>使用模型进行预测</li>
<li>计算损失</li>
<li><code>loss.backward()</code> 更新模型的梯度，即权重和偏置</li>
</ul>
<p>我们现在使用这些梯度来更新权重和偏置。我们将在 <code>torch.no_grad()</code> 中执行，因为<br>我们不想记录这些操作来进行下一次梯度计算。</p>
<p>我们随后将梯度设置为0，以便为下一次循环做好准备。否则，梯度将会记录所有发生的操作。（也就是说， <code>loss.backward()</code> 将梯度增加到已经存在的值上，而不是替代它）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from IPython.core.debugger import set_trace</span><br><span class="line"></span><br><span class="line">lr = 0.5  ### learning rate</span><br><span class="line">epochs = 2  ### how many epochs to train for</span><br><span class="line"></span><br><span class="line">for epoch in range(epochs):</span><br><span class="line">    for i in range((n - 1) // bs + 1):</span><br><span class="line">       ###set_trace()</span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            weights -= weights.grad * lr</span><br><span class="line">            bias -= bias.grad * lr</span><br><span class="line">            weights.grad.zero_()</span><br><span class="line">            bias.grad.zero_()</span><br></pre></td></tr></table></figure>
<p>到此，我们已经从头编写并训练了一个小型的神经网络。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.0806, grad_fn=&lt;negbackward&gt;) tensor(1.)</span><br><span class="line">&lt;/negbackward&gt;</span><br></pre></td></tr></table></figure>
<p>将损失和精确度与前边的比较，发现损失减少，精确度提升。</p>
<h3 id="3、使用-torch-nn-functional"><a href="#3、使用-torch-nn-functional" class="headerlink" title="3、使用 torch.nn.functional"></a>3、使用 <code>torch.nn.functional</code></h3><p>我们现在来重构代码，代码的功能和前边的一样，我们只是利用 <code>PyTorch</code> 的 <code>nn</code> 类来使得代码更简洁和灵活。</p>
<p>第一步并且最简单的一步是用 <code>torch.nn.functional</code>（通常导入到命名空间F中）中的函数替代我们手工编写的激活函数和损失函数来缩短代码。该模块包含 <code>torch.nn</code> 库中所有的函数（而库的其他部分还包含类）。除了各种损失函数和激活函数，在还模块中你还可以发现许多用于创建神经网络的方便的函数，如池化函数等。</p>
<p>如果使用了负对数似然损失函数和 <code>log softnax</code> 激活函数，那么Pytorch提供的 <code>F.cross_entropy</code> 结合了两者。所以我们甚至可以从我们的模型中移除激活函数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">loss_func = F.cross_entropy</span><br><span class="line"></span><br><span class="line">def model(xb):</span><br><span class="line">    return xb @ weights + bias</span><br></pre></td></tr></table></figure>
<p>注意，在 <code>model</code> 函数中我们不再需要调用 <code>log_softmax</code>。让我们确认一下，损失和精确度与前边计算的一样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.0806, grad_fn=&lt;nlllossbackward&gt;) tensor(1.)</span><br><span class="line">&lt;/nlllossbackward&gt;</span><br></pre></td></tr></table></figure>
<h3 id="4、使用-nn-Module-重构"><a href="#4、使用-nn-Module-重构" class="headerlink" title="4、使用 nn.Module 重构"></a>4、使用 <code>nn.Module</code> 重构</h3><p>下一步，我们将使用 <code>nn.Module</code> 和 <code>nn.Parameter</code>,以获得更清晰更简洁的训练循环。我们继承 <code>nn.Module</code>（它本身是一个类并且能够跟踪状态）建立子类。我们想要建立一个包含权重、偏置和前向传播的方法的类。 <code>nn.Module</code> 拥有许多我们将会使用的属性和方法（例如： <code>.parameters()</code> 和 <code>.zero_grad()</code>）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">class Mnist_Logistic(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(10))</span><br><span class="line"></span><br><span class="line">    def forward(self, xb):</span><br><span class="line">        return xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>
<p>因为我们现在使用一个对象而不是一个函数，所以我们首先需要实例化我们的模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br></pre></td></tr></table></figure>
<p>现在我们可以像之前那样计算损失。注意， <code>nn.Module</code> 对象像是函数一样被使用（即它们能被调用），但在幕后， <code>PyTorch</code> 将自动调用我们的 <code>forward</code> 方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(2.3558, grad_fn=&lt;nlllossbackward&gt;)</span><br><span class="line">&lt;/nlllossbackward&gt;</span><br></pre></td></tr></table></figure>
<p>以前对于我们的训练循环，我们需要按名字更新每个参数的值，并且手动将每个参数的梯度归零，像下面这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    weights -= weights.grad * lr</span><br><span class="line">    bias -= bias.grad * lr</span><br><span class="line">    weights.grad.zero_()</span><br><span class="line">    bias.grad.zero_()</span><br></pre></td></tr></table></figure>
<p>现在我们可以利用 <code>model.paremeters()</code> 和 <code>model.zero_grad()</code> 使得这些步骤更简洁，特别是，当我们有一个更复杂的模型时，使得我们更不容忘记某些参数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    for p in model.parameters(): p -= p.grad * lr</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<p>我们将训练循环包装到一个 <code>fit</code> 函数中，以便我们以后运行。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def fit():</span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        for i in range((n - 1) // bs + 1):</span><br><span class="line">            start_i = i * bs</span><br><span class="line">            end_i = start_i + bs</span><br><span class="line">            xb = x_train[start_i:end_i]</span><br><span class="line">            yb = y_train[start_i:end_i]</span><br><span class="line">            pred = model(xb)</span><br><span class="line">            loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            with torch.no_grad():</span><br><span class="line">                for p in model.parameters():</span><br><span class="line">                    p -= p.grad * lr</span><br><span class="line">                model.zero_grad()</span><br><span class="line"></span><br><span class="line">fit()</span><br></pre></td></tr></table></figure>
<p>再次检查损失是否下降</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.0826, grad_fn=&lt;nlllossbackward&gt;)</span><br><span class="line">&lt;/nlllossbackward&gt;</span><br></pre></td></tr></table></figure>
<h3 id="5、使用-nn-Linear-重构"><a href="#5、使用-nn-Linear-重构" class="headerlink" title="5、使用 nn.Linear 重构"></a>5、使用 <code>nn.Linear</code> 重构</h3><p>继续重构代码。我们将会使用PyTorch 的 <code>nn.Linear</code> 类建立一个线性层，以替代手动定义和初始化 <code>self.weights</code> 和 <code>self.bias</code>、计算 <code>xb @ self.weights + self.bias</code> 等工作。PyTorch拥有多中类型预先定义好的层可以帮助我们极大简化代码，并且通常可以使之运行更快。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class Mnist_Logistic(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.lin = nn.Linear(784, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, xb):</span><br><span class="line">        return self.lin(xb)</span><br></pre></td></tr></table></figure>
<p>我们像之前一样实例化模型并且计算损失</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(2.3156, grad_fn=&lt;nlllossbackward&gt;)</span><br><span class="line">&lt;/nlllossbackward&gt;</span><br></pre></td></tr></table></figure>
<p>我们仍然能够像之前那样使用 <code>fit</code> 方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fit()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.0809, grad_fn=&lt;nlllossbackward&gt;)</span><br><span class="line">&lt;/nlllossbackward&gt;</span><br></pre></td></tr></table></figure>
<h3 id="6、使用-optim-重构"><a href="#6、使用-optim-重构" class="headerlink" title="6、使用 optim 重构"></a>6、使用 <code>optim</code> 重构</h3><p>PyTorch还有一个包含各种优化算法的包 <code>torch.optim</code> 。我们可以使用优化器中的 <code>step</code> 方法来执行前向步骤，而不是手动更新参数。</p>
<p>这将使得我们替换之前手动编写的优化步骤：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    for p in model.parameters(): p -= p.grad * lr</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<p>替换为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">opt.step()</span><br><span class="line">opt.zero_grad()</span><br></pre></td></tr></table></figure>
<p><code>optim.zero_grad()</code> 将梯度重置为0，我们需要在计算下一个minibatch的梯度前调用它。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from torch import optim</span><br></pre></td></tr></table></figure>
<p>我们将要定义一个函数来创建模型和优化器，以便将来可以重用它。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def get_model():</span><br><span class="line">    model = Mnist_Logistic()</span><br><span class="line">    return model, optim.SGD(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">model, opt = get_model()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br><span class="line"></span><br><span class="line">for epoch in range(epochs):</span><br><span class="line">    for i in range((n - 1) // bs + 1):</span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor(2.2861, grad_fn=&lt;nlllossbackward&gt;)</span><br><span class="line">tensor(0.0815, grad_fn=&lt;nlllossbackward&gt;)</span><br><span class="line">&lt;/nlllossbackward&gt;&lt;/nlllossbackward&gt;</span><br></pre></td></tr></table></figure>
<h3 id="7、使用-Dataset-重构"><a href="#7、使用-Dataset-重构" class="headerlink" title="7、使用 Dataset 重构"></a>7、使用 <code>Dataset</code> 重构</h3><p><code>PyTorch</code> 有一个抽象的 <code>Dataset</code> 类。任何具有 <code>__len__</code>（通过Python的标准len函数调用）函数和 <code>__getitem__</code> 函数的类都可以是一个 <code>Dataset</code>。</p>
<p>Pytorch 的 <code>TensorDataset</code> 是一个包装 <code>tensor</code> 的 <code>Dataset</code>。通过定义长度和索引方式，这也为我们提供了一种沿tensor第一维迭代、索引、切片的方法。这将使我们更容易在我们训练的同一行中访问独立变量和因变量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.data import TensorDataset</span><br></pre></td></tr></table></figure>
<p><code>x_train</code> 和 <code>y_train</code> 可以组合在一个单独的 <code>TensorDataset</code> 中，这将更容易迭代和切片。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br></pre></td></tr></table></figure>
<p>之前，我们必须分别迭代x和y的小批量值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xb = x_train[start_i:end_i]</span><br><span class="line">yb = y_train[start_i:end_i]</span><br></pre></td></tr></table></figure>
<p>现在，我们可以一起做这两步：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xb,yb = train_ds[i*bs : i*bs+bs]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line">for epoch in range(epochs):</span><br><span class="line">    for i in range((n - 1) // bs + 1):</span><br><span class="line">        xb, yb = train_ds[i * bs: i * bs + bs]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.0800, grad_fn=&lt;nlllossbackward&gt;)</span><br><span class="line">&lt;/nlllossbackward&gt;</span><br></pre></td></tr></table></figure>
<h3 id="8、使用-DataLoader-重构"><a href="#8、使用-DataLoader-重构" class="headerlink" title="8、使用 DataLoader 重构"></a>8、使用 <code>DataLoader</code> 重构</h3><p>PyTorch 的 <code>DataLoader</code> 负责管理批次。你可以从任何 <code>Dataset</code> 创建 <code>DataLoader</code>。 <code>DataLoader</code> 使得迭代批次更简单。 <code>DataLoader</code> 自动地提供每个批次，而不必使用 <code>train_ds[i*bs : i*bs+bs]</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"></span><br><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs)</span><br></pre></td></tr></table></figure>
<p>之前，我们像下面这样迭代批次：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for i in range((n-1)//bs + 1):</span><br><span class="line">    xb,yb = train_ds[i*bs : i*bs+bs]</span><br><span class="line">    pred = model(xb)</span><br></pre></td></tr></table></figure>
<p>现在，我们的循环更加简洁，因为 <code>&amp;###xFF08;xb&amp;###xFF0C;yb&amp;###xFF09;</code>可以自动从 <code>DataLoader</code> 自动加载：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for xb,yb in train_dl:</span><br><span class="line">    pred = model(xb)</span><br><span class="line">model, opt = get_model()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(epochs):</span><br><span class="line">    for xb, yb in train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.0821, grad_fn=&lt;nlllossbackward&gt;)</span><br><span class="line">&lt;/nlllossbackward&gt;</span><br></pre></td></tr></table></figure>
<p>由于PyTorch的 <code>nn.Module</code>、 <code>nn.Parameter</code>、 <code>Dataset</code> 和 <code>DataLoader</code>，现在我们的训练循环变得更小、更容易理解。现在让我们尝试添加在实践中创建有效模型所需的基本功能。</p>
<h3 id="9、增加验证"><a href="#9、增加验证" class="headerlink" title="9、增加验证"></a>9、增加验证</h3><p>在第1部分中，我们只是尝试去设置合理的训练循环以用于我们的训练数据。 实际上，你总是应该有一个验证集，以确定你是否过度拟合。</p>
<p>打乱训练数据对于防止批次与过度拟合之间的相关性非常重要。 另一方面，无论我们是否打乱验证集，验证损失都是相同的。 由于打乱需要额外的时间，因此打乱验证数据是没有意义的。</p>
<p>我们将要使用的验证集的大小是训练集的两倍。这是因为验证集不需要反向传播，因此占用更少的内存（不需要存储梯度）。 我们利用这一点来使用更大的批次大小并更快地计算损失。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)</span><br><span class="line"></span><br><span class="line">valid_ds = TensorDataset(x_valid, y_valid)</span><br><span class="line">valid_dl = DataLoader(valid_ds, batch_size=bs * 2)</span><br></pre></td></tr></table></figure>
<p>我们将在每个epoch结束时计算和打印验证损失。（注意，我们总是在训练之前调用 <code>model.train()</code>，在推理之前调用 <code>model.eval()</code>，因为这些由诸如 <code>nn.BatchNorm2d</code> 和 <code>nn.Dropout</code> 等层使用，以确保这些不同阶段的适当行为。）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line">for epoch in range(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    for xb, yb in train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    model.eval()</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)</span><br><span class="line"></span><br><span class="line">    print(epoch, valid_loss / len(valid_dl))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0 tensor(0.2981)</span><br><span class="line">1 tensor(0.3033)</span><br></pre></td></tr></table></figure>
<h3 id="10、创建-fit-和-get-data"><a href="#10、创建-fit-和-get-data" class="headerlink" title="10、创建 fit() 和 get_data()"></a>10、创建 <code>fit()</code> 和 <code>get_data()</code></h3><p>我们现在将进行一些重构。因为计算训练集和验证集的损失，我们进行了两次相似的处理，让我们将其作为一个 <code>loss_batch</code> 函数来计算每个批次的损失。</p>
<p>我们为训练集传递一个优化器，并使用它来执行反向传播。 对于验证集，我们不传递优化器，因此该方法不执行反向传播。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def loss_batch(model, loss_func, xb, yb, opt=None):</span><br><span class="line">    loss = loss_func(model(xb), yb)</span><br><span class="line"></span><br><span class="line">    if opt is not None:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    return loss.item(), len(xb)</span><br></pre></td></tr></table></figure>
<p><code>fit</code> 运行必要的操作来训练我们的模型并计算每个epoch的训练和验证损失。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def fit(epochs, model, loss_func, opt, train_dl, valid_dl):</span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        for xb, yb in train_dl:</span><br><span class="line">            loss_batch(model, loss_func, xb, yb, opt)</span><br><span class="line"></span><br><span class="line">        model.eval()</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            losses, nums = zip(</span><br><span class="line">                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]</span><br><span class="line">            )</span><br><span class="line">        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)</span><br><span class="line"></span><br><span class="line">        print(epoch, val_loss)</span><br></pre></td></tr></table></figure>
<p><code>get_data</code> 为训练集合验证集返回 <code>DataLoader</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def get_data(train_ds, valid_ds, bs):</span><br><span class="line">    return (</span><br><span class="line">        DataLoader(train_ds, batch_size=bs, shuffle=True),</span><br><span class="line">        DataLoader(valid_ds, batch_size=bs * 2),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>现在，我们获取 <code>DataLoader</code> 和拟合模型的整个过程可以在3行代码中运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">model, opt = get_model()</span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0 0.3055081913471222</span><br><span class="line">1 0.31777948439121245</span><br></pre></td></tr></table></figure>
<h3 id="11、总结"><a href="#11、总结" class="headerlink" title="11、总结"></a>11、总结</h3><p>我们现在有一个通用数据流水线和训练循环，你可以使用它来训练多种类型PyTorch模型。 各部分的功能总结如下：</p>
<ul>
<li><code>torch.nn</code><ul>
<li><code>Module</code>：创建一个可调用的对象，其行为类似于一个函数，但也可以包含状态（例如神经网络层权重）。 它知道它包含哪些参数，并且可以将所有梯度归零，循环遍历它们更新权重等。</li>
<li><code>Parameter</code>： <code>tensor</code> 的包装器（wrapper），它告诉 <code>Module</code> 它具有在反向传播期间需要更新的权重。 只更新具有 <code>requires_grad</code> 属性的 <code>tensor</code>。</li>
<li><code>functional</code>：一个模块（通常按惯例导入到F命名空间中），它包含激活函数，损失函数等，以及非状态（non-stateful）版本的层，如卷积层和线性层。</li>
</ul>
</li>
<li><code>torch.optim</code>：包含 <code>SGD</code> 等优化器，可在后向传播步骤中更新 <code>Parameter</code> 的权重。</li>
<li><code>Dataset</code>：带有 <code>__len__</code> 和 <code>__getitem__</code> 的对象的抽象接口，包括 <code>PyTorch</code> 提供的类，如 <code>TensorDataset</code>。</li>
<li><code>DataLoader</code>：获取任何 <code>Dataset</code> 并创建一个返回批量数据的迭代器。</li>
</ul>
<p>参考文章：</p>
<ol>
<li><a href="https://blog.csdn.net/Spring_24/article/details/100128412" target="_blank" rel="noopener">https://blog.csdn.net/Spring_24/article/details/100128412</a></li>
<li><a href="https://blog.csdn.net/Spring_24/article/details/100170304" target="_blank" rel="noopener">https://blog.csdn.net/Spring_24/article/details/100170304</a></li>
<li><a href="https://www.cnblogs.com/xianhan/p/9145966.html" target="_blank" rel="noopener">https://www.cnblogs.com/xianhan/p/9145966.html</a></li>
</ol>

    </div>

    
    
    

	
        
      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Snow</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://behappy00.github.io/2021/04/16/torch-nn学习-1/" title="torch.nn学习(1)">https://behappy00.github.io/2021/04/16/torch-nn学习-1/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/转载/" rel="tag"># 转载</a>
            
              <a href="/tags/python/" rel="tag"># python</a>
            
              <a href="/tags/pytorch/" rel="tag"># pytorch</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2021/04/12/论文阅读-label-wise-attention/" rel="next" title="论文阅读__label-wise-attention">
                  <i class="fa fa-chevron-left"></i> 论文阅读__label-wise-attention
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2021/04/17/深度学习笔记——RNN（LSTM、GRU、双向RNN）学习总结/" rel="prev" title="深度学习笔记——RNN（LSTM、GRU、双向RNN）学习总结">
                  深度学习笔记——RNN（LSTM、GRU、双向RNN）学习总结 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
		
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1、设置MNIST数据"><span class="nav-number">1.</span> <span class="nav-text">1、设置MNIST数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2、从头构建神经网络（不使用-torch-nn-）"><span class="nav-number">2.</span> <span class="nav-text">2、从头构建神经网络（不使用 torch.nn ）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3、使用-torch-nn-functional"><span class="nav-number">3.</span> <span class="nav-text">3、使用 torch.nn.functional</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4、使用-nn-Module-重构"><span class="nav-number">4.</span> <span class="nav-text">4、使用 nn.Module 重构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5、使用-nn-Linear-重构"><span class="nav-number">5.</span> <span class="nav-text">5、使用 nn.Linear 重构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6、使用-optim-重构"><span class="nav-number">6.</span> <span class="nav-text">6、使用 optim 重构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7、使用-Dataset-重构"><span class="nav-number">7.</span> <span class="nav-text">7、使用 Dataset 重构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8、使用-DataLoader-重构"><span class="nav-number">8.</span> <span class="nav-text">8、使用 DataLoader 重构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9、增加验证"><span class="nav-number">9.</span> <span class="nav-text">9、增加验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10、创建-fit-和-get-data"><span class="nav-number">10.</span> <span class="nav-text">10、创建 fit() 和 get_data()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11、总结"><span class="nav-number">11.</span> <span class="nav-text">11、总结</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Snow</p>
  <div class="site-description" itemprop="description">计算机 Hiter</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">147</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">53</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



      </div>

	 <!-- 添加近期文章 -->
	 
	   <div class="links-of-blogroll motion-element links-of-blogroll-block">
		 <div class="links-of-blogroll-title">
		   <!-- modify icon to fire by szw -->
		   <i class="fa fa-history fa-" aria-hidden="true"></i>
		   近期文章
		 </div>
		 <ul class="links-of-blogroll-list">
		   
		   
			 <li>
			   <a href="/2023/10/12/BT知识科普/" title="BT知识科普" target="_blank">BT知识科普</a>
			 </li>
		   
			 <li>
			   <a href="/2023/10/03/Java截取（提取）子字符串/" title="Java截取（提取）子字符串" target="_blank">Java截取（提取）子字符串</a>
			 </li>
		   
			 <li>
			   <a href="/2023/10/03/java读取文件更改并生成另一个文件/" title="java读取文件更改并生成另一个文件" target="_blank">java读取文件更改并生成另一个文件</a>
			 </li>
		   
			 <li>
			   <a href="/2023/10/03/java中IO流详细解释/" title="java中IO流详细解释" target="_blank">java中IO流详细解释</a>
			 </li>
		   
			 <li>
			   <a href="/2023/10/03/markdown汇总/" title="markdown汇总" target="_blank">markdown汇总</a>
			 </li>
		   
		 </ul>
	   </div>
	 
	  
	  <!-- 添加网易云播放歌曲 -->
	  <div id="music163player">
		<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=110 src="//music.163.com/outchain/player?type=0&id=5457006033&auto=0&height=90"></iframe>
		</iframe>
	  </div>
  
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2023</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Snow</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">573k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8:41</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="我的第 undefined 位朋友，">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="历经 undefined 次回眸才与你相遇">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@2.1.3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>



  








  <script src="/js/local-search.js?v=7.4.0"></script>














  

  

  

  


  <!--

<script>
  $(document).ready(function () {
    $(".header-inner").animate({padding: "25px 0 25px"}, 1000);
  });
</script>



  <script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
  <script>
    function timer() {
      var ages = moment.preciseDiff(moment(),moment(20190628,"YYYYMMDD"));
      ages = ages.replace(/years?/, "年");
      ages = ages.replace(/months?/, "月");
      ages = ages.replace(/days?/, "天");
      ages = ages.replace(/hours?/, "小时");
      ages = ages.replace(/minutes?/, "分");
      ages = ages.replace(/seconds?/, "秒");
      ages = ages.replace(/\d+/g, '<span style="color:#1094e8">$&</span>');
      div.innerHTML = `我已在此等候你 ${ages}`;
    }
    var div = document.createElement("div");
    //插入到copyright之后
    var copyright = document.querySelector(".copyright");
    document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
    timer();
    setInterval("timer()",1000)
  </script>
-->

<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'nClDgsEbJJCRM7oMK99qaGCi-MdYXbMMI',
    appKey: 'IHpqCV0C6qKHHw7TnYT4gHHl',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: 'zh-CN' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

  
  <!--页面点击出现富强民主文明和谐-->
 <script type="text/javascript" src="/js/words-click.js"></script > 

  <!-- 代码块复制功能 -->
  <script type="text/javascript" src="/js/src/clipboard.min.js"></script>  
  <!--
  <script type="text/javascript" src="/js/src/clipboard-use.js"></script>
  -->
  
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true,"scale":0.05},"react":{"opacityDefault":0.7,"opacityOnHover":0.2}});</script></body>
</html>

